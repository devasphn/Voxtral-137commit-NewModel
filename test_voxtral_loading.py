#!/usr/bin/env python3
"""
Voxtral Model Loading Diagnostic Script
Tests the fixed model loading with comprehensive error reporting
"""

import os
import sys
import traceback
import time
import soundfile as sf  # ADDED: Import soundfile for audio file operations
from typing import Optional

def test_environment():
    """Test environment setup"""
    print("üîç TESTING ENVIRONMENT SETUP")
    print("=" * 50)
    
    # Check HF_TOKEN
    hf_token = os.getenv('HF_TOKEN')
    if hf_token:
        print(f"‚úÖ HF_TOKEN is set (length: {len(hf_token)})")
    else:
        print("‚ùå HF_TOKEN is not set!")
        print("   Set it with: export HF_TOKEN='your_token_here'")
        return False
    
    # Check CUDA
    try:
        import torch
        cuda_available = torch.cuda.is_available()
        if cuda_available:
            print(f"‚úÖ CUDA available: {torch.cuda.get_device_name(0)}")
            print(f"   CUDA version: {torch.version.cuda}")
            print(f"   GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
        else:
            print("‚ö†Ô∏è CUDA not available, will use CPU")
    except Exception as e:
        print(f"‚ùå Error checking CUDA: {e}")
    
    # Check cache directory
    cache_dir = "./model_cache"
    if os.path.exists(cache_dir):
        print(f"‚úÖ Cache directory exists: {cache_dir}")
    else:
        print(f"üìÅ Creating cache directory: {cache_dir}")
        os.makedirs(cache_dir, exist_ok=True)
    
    print()
    return True

def test_imports():
    """Test required imports"""
    print("üîç TESTING IMPORTS")
    print("=" * 50)
    
    try:
        import transformers
        print(f"‚úÖ transformers version: {transformers.__version__}")
    except ImportError as e:
        print(f"‚ùå transformers import failed: {e}")
        return False
    
    try:
        from transformers import VoxtralForConditionalGeneration, AutoProcessor
        print("‚úÖ Voxtral classes imported successfully")
    except ImportError as e:
        print(f"‚ùå Voxtral classes import failed: {e}")
        print("   Install with: pip install transformers>=4.56.0")
        return False
    
    try:
        import torch
        print(f"‚úÖ torch version: {torch.__version__}")
    except ImportError as e:
        print(f"‚ùå torch import failed: {e}")
        return False
    
    print()
    return True

def test_processor_loading():
    """Test AutoProcessor loading"""
    print("üîç TESTING AUTOPROCESSOR LOADING")
    print("=" * 50)
    
    try:
        from transformers import AutoProcessor
        
        processor_kwargs = {
            "cache_dir": "./model_cache",
            "trust_remote_code": True
        }
        
        # Add token if available
        if os.getenv('HF_TOKEN'):
            processor_kwargs["token"] = os.getenv('HF_TOKEN')
            print("üîë Using HuggingFace authentication token")
        
        print("üîÑ Loading AutoProcessor...")
        start_time = time.time()
        
        processor = AutoProcessor.from_pretrained(
            "mistralai/Voxtral-Mini-3B-2507",
            **processor_kwargs
        )
        
        load_time = time.time() - start_time
        print(f"‚úÖ AutoProcessor loaded successfully in {load_time:.2f}s")
        print(f"   Processor type: {type(processor).__name__}")
        
        return processor
        
    except Exception as e:
        print(f"‚ùå AutoProcessor loading failed: {e}")
        print("\nFull traceback:")
        traceback.print_exc()
        return None

def test_model_loading():
    """Test Voxtral model loading with fallbacks"""
    print("üîç TESTING VOXTRAL MODEL LOADING")
    print("=" * 50)
    
    try:
        from transformers import VoxtralForConditionalGeneration
        import torch
        
        # Primary loading attempt
        model_kwargs = {
            "cache_dir": "./model_cache",
            "dtype": torch.float16,
            "device_map": "auto",
            "low_cpu_mem_usage": True,
            "trust_remote_code": True,
            "attn_implementation": "flash_attention_2",
            "use_safetensors": True,
        }
        
        # Add token if available
        if os.getenv('HF_TOKEN'):
            model_kwargs["token"] = os.getenv('HF_TOKEN')
            print("üîë Using HuggingFace authentication token")
        
        print("üîÑ Loading Voxtral model (primary attempt)...")
        start_time = time.time()
        
        try:
            model = VoxtralForConditionalGeneration.from_pretrained(
                "mistralai/Voxtral-Mini-3B-2507",
                **model_kwargs
            )
            load_time = time.time() - start_time
            print(f"‚úÖ Voxtral model loaded successfully in {load_time:.2f}s")
            print(f"   Model device: {model.device}")
            print(f"   Model dtype: {model.dtype}")
            print(f"   Attention: flash_attention_2")
            return model
            
        except Exception as primary_error:
            print(f"‚ö†Ô∏è Primary loading failed: {primary_error}")
            
            # Fallback 1: Eager attention
            print("üîÑ Trying fallback 1: eager attention...")
            model_kwargs["attn_implementation"] = "eager"
            
            try:
                model = VoxtralForConditionalGeneration.from_pretrained(
                    "mistralai/Voxtral-Mini-3B-2507",
                    **model_kwargs
                )
                load_time = time.time() - start_time
                print(f"‚úÖ Model loaded with eager attention in {load_time:.2f}s")
                return model
                
            except Exception as eager_error:
                print(f"‚ö†Ô∏è Eager attention failed: {eager_error}")
                
                # Fallback 2: No safetensors
                print("üîÑ Trying fallback 2: no safetensors...")
                model_kwargs["use_safetensors"] = False
                
                try:
                    model = VoxtralForConditionalGeneration.from_pretrained(
                        "mistralai/Voxtral-Mini-3B-2507",
                        **model_kwargs
                    )
                    load_time = time.time() - start_time
                    print(f"‚úÖ Model loaded without safetensors in {load_time:.2f}s")
                    return model
                    
                except Exception as safetensors_error:
                    print(f"‚ùå All fallbacks failed: {safetensors_error}")
                    raise safetensors_error
        
    except Exception as e:
        print(f"‚ùå Model loading completely failed: {e}")
        print("\nFull traceback:")
        traceback.print_exc()
        return None

def test_model_inference(model, processor):
    """Test basic model inference using correct VoxtralProcessor API"""
    print("üîç TESTING MODEL INFERENCE")
    print("=" * 50)

    try:
        import torch
        import numpy as np
        import tempfile
        import soundfile as sf

        # Create dummy audio input
        sample_rate = 16000
        duration = 1.0  # 1 second
        audio_data = np.random.randn(int(sample_rate * duration)).astype(np.float32)

        print("üîÑ Testing model inference with correct VoxtralProcessor API...")
        start_time = time.time()

        # FIXED: Use correct VoxtralProcessor API with conversation format
        # Save audio to temporary file (required for VoxtralProcessor)
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
            sf.write(tmp_file.name, audio_data, sample_rate)
            audio_path = tmp_file.name

        # Test both audio+text and audio-only modes
        print("   Testing audio+text mode...")
        conversation_with_text = [
            {
                "role": "user",
                "content": [
                    {"type": "audio", "path": audio_path},
                    {"type": "text", "text": "What can you tell me about this audio?"}
                ]
            }
        ]

        # Process using apply_chat_template (correct API)
        inputs = processor.apply_chat_template(conversation_with_text, return_tensors="pt")

        # Move to same device as model
        if hasattr(model, 'device'):
            inputs = inputs.to(model.device)

        # Generate response
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=10,
                do_sample=True,
                temperature=0.1,
                use_cache=True
            )

        # Decode response
        decoded_outputs = processor.batch_decode(
            outputs[:, inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )

        print(f"   ‚úÖ Audio+text mode successful")
        print(f"   Generated response: {decoded_outputs[0][:50]}...")

        # Test audio-only mode (for speech-to-speech)
        print("   Testing audio-only mode (speech-to-speech)...")
        conversation_audio_only = [
            {
                "role": "user",
                "content": [
                    {"type": "audio", "path": audio_path}
                ]
            }
        ]

        # Process audio-only
        inputs = processor.apply_chat_template(conversation_audio_only, return_tensors="pt")

        # Move to same device as model
        if hasattr(model, 'device'):
            inputs = inputs.to(model.device)

        # Generate response for audio-only
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=10,
                do_sample=True,
                temperature=0.1,
                use_cache=True
            )

        # Decode response
        decoded_outputs = processor.batch_decode(
            outputs[:, inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )

        inference_time = time.time() - start_time
        print(f"   ‚úÖ Audio-only mode successful")
        print(f"   Generated response: {decoded_outputs[0][:50]}...")

        print(f"‚úÖ Model inference successful in {inference_time:.3f}s")
        print(f"   Both audio+text and audio-only modes working")
        print(f"   Input shape: {inputs.input_ids.shape}")
        print(f"   Output shape: {outputs.shape}")

        # Clean up temporary file
        import os
        os.unlink(audio_path)

        return True

    except Exception as e:
        print(f"‚ùå Model inference failed: {e}")
        print("\nFull traceback:")
        traceback.print_exc()
        return False

def main():
    """Main diagnostic function"""
    print("üöÄ VOXTRAL MODEL LOADING DIAGNOSTIC")
    print("=" * 60)
    print()
    
    # Test environment
    if not test_environment():
        print("‚ùå Environment test failed. Please fix issues above.")
        return False
    
    # Test imports
    if not test_imports():
        print("‚ùå Import test failed. Please install required packages.")
        return False
    
    # Test processor loading
    processor = test_processor_loading()
    if processor is None:
        print("‚ùå Processor loading failed. Check authentication and network.")
        return False
    
    # Test model loading
    model = test_model_loading()
    if model is None:
        print("‚ùå Model loading failed. Check logs above for details.")
        return False
    
    # Test inference
    if not test_model_inference(model, processor):
        print("‚ùå Model inference failed. Model may be corrupted.")
        return False
    
    print()
    print("üéâ ALL TESTS PASSED!")
    print("‚úÖ Voxtral model loading is working correctly")
    print("‚úÖ Your speech-to-speech system should now work")
    print()
    
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
