# Configuration for Voxtral + Direct Orpheus TTS Integration
# Optimized for sub-300ms end-to-end latency with direct model integration

server:
  host: "0.0.0.0"
  http_port: 8000
  health_port: 8005
  tcp_ports: [8765, 8766]

model:
  name: "mistralai/Voxtral-Mini-3B-2507"
  cache_dir: "/workspace/model_cache"
  device: "cuda"
  torch_dtype: "bfloat16"
  max_memory_per_gpu: "8GB"

audio:
  sample_rate: 16000
  chunk_size: 1024
  format: "int16"
  channels: 1
  frame_duration_ms: 30

spectrogram:
  n_mels: 128
  hop_length: 160
  win_length: 400
  n_fft: 400

streaming:
  max_connections: 100
  buffer_size: 4096
  timeout_seconds: 300
  latency_target_ms: 300  # Sub-300ms end-to-end target

logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "/workspace/logs/voxtral_streaming.log"

# TTS Configuration - Direct Orpheus Integration
tts:
  engine: "orpheus-direct"  # Use direct integration instead of FastAPI
  default_voice: "ऋतिका"   # Hindi female voice as primary
  sample_rate: 24000
  enabled: true
  
  # Direct Orpheus Model Configuration
  orpheus_direct:
    model_name: "canopy-ai/Orpheus-3b"  # Correct Orpheus model
    device: "cuda"
    torch_dtype: "float16"
    max_new_tokens: 1000
    temperature: 0.1
    top_p: 0.95
  
  # Legacy Orpheus-FastAPI Server Configuration (disabled)
  orpheus_server:
    host: "localhost"
    port: 1234
    timeout: 30
    model_path: "/workspace/models/Orpheus-3b-FT-Q8_0.gguf"
    enabled: false  # Disabled in favor of direct integration
  
  # Voice Configuration
  voices:
    english: ["tara", "leah", "jess", "leo", "dan", "mia", "zac", "zoe"]
    french: ["pierre", "amelie", "marie"]
    german: ["jana", "thomas", "max"]
    korean: ["유나", "준서"]
    hindi: ["ऋतिका"]
    mandarin: ["长乐", "白芷"]
    spanish: ["javi", "sergio", "maria"]
    italian: ["pietro", "giulia", "carlo"]
  
  # Performance Configuration
  performance:
    batch_size: 1  # Direct integration uses single batch
    max_queue_size: 32
    num_workers: 4
    target_latency_ms: 150  # Target for TTS generation
    memory_optimization: "balanced"  # "performance", "balanced", "memory_efficient"
  
  # GPU Memory Management
  gpu_memory:
    min_vram_gb: 8.0
    recommended_vram_gb: 16.0
    memory_fraction: 0.9
    cleanup_frequency: "after_each_generation"
    enable_monitoring: true

# Performance Monitoring Configuration
performance:
  enable_monitoring: true
  
  # Latency Targets (milliseconds)
  latency_targets:
    voxtral_processing_ms: 100    # Voxtral STT + LLM processing
    orpheus_generation_ms: 150    # Orpheus TTS generation
    audio_conversion_ms: 50       # SNAC audio conversion
    total_end_to_end_ms: 300      # Total pipeline latency
  
  # Alert Thresholds
  alert_thresholds:
    consecutive_failures: 5       # Alert after N consecutive failures
    degradation_threshold: 1.5    # Alert if latency increases by 50%
    success_rate_threshold: 0.8   # Alert if success rate drops below 80%
  
  optimization_level: "balanced"  # "performance", "balanced", "memory_efficient"

# Environment-specific overrides
# These can be set via environment variables using the pattern:
# VOXTRAL__SECTION__SUBSECTION__KEY (double underscores for nesting)
#
# Examples:
# VOXTRAL__SERVER__HTTP_PORT=8080
# VOXTRAL__TTS__DEFAULT_VOICE=tara
# VOXTRAL__PERFORMANCE__LATENCY_TARGETS__TOTAL_END_TO_END_MS=250